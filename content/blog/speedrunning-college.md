---
title: "Speedrunning College"
subtitle: "The underrated third option"
date: 2024-03-29T09:10:05-06:00
---

A few decades ago, universities were the only place where it was possible for students to use and learn about computers;
now it's possible to teach yourself the entire curriculum without leaving your basement.
This has led to the popular social-media trope of "first line of code to FAANG engineer in X months", and the frequent discussion of "college vs bootcamp vs self-taught".

My experience in college and the job market has led me to the conclusion that both the traditional 4-year degree and the modern self-taught-developer path are both deeply flawed, and that, for many people, getting a 4-year degree in less than 4 years is the optimal choice.

## College: The Downsides

### Cost

This is the obvious one.
Thousands of dollars in debt is a sorry way to start a career.
If somebody's paying for your education, then speedrunning probably isn't the ideal path, unless you're looking for a challenge or you resonate strongly with the other downsides.
However, for most students, more time in school means more debt, and a faster graduation is a direct path to greater financial security.

### Format

At its core, school is an abstraction over the real world: it sets up an artificial environment to simulate the requirements and rewards of the workplace, with the hope of having learning as the primary outcome rather than a side-effect.
The abstraction is loose, though, and it has a tendency to shift and stretch over time due to its loose bound to the environment it's attempting to emulate and the heavy influence from the students/faculty/administration who tend to mold it to fit their own selfish interests.
This leads to a multitude of problems:

- Course requirements are completely arbitrary
    - Learning objectives are often criticized as too abstract (not practical enough)
        - "I'll never use this in the real world"
    - Standards lower over time (grade inflation)
    - Based on what a professor (often from an older generation) thinks will be beneficial
        - This often morphs into what the professor wants to teach, in the format that's most convenient for them, rather than what's most important and effective for students
- Unbalanced requirements and rewards
    - Getting good grades is not very rewarding
        - Employers don't weigh grades highly
            - This probably has to do with grade inflation (it's easier to get good grades now)
        - Little internal (given by the university) incentive/advantages gained
        - The main goal shifts to avoiding bad performance rather than pursuing excellence
            - This does not reflect the real world, where, though negative forces are often stronger than positive ones, hard work over time almost always leads to an upward spiral
            - This trains an unhealthy mind-set that encourages suboptimal performance
    - The difficulty of classes is not balanced
        - The number of credits (or the requirements a course fills in general) often aren't well aligned with the actual class
        - This leads to a huge emphasis on which professor is teaching a class, and previous years' grade distributions
            - This is a major distraction from the ideal focus: the actual topic of the class
- Inaccuracy of assessment
    - Exams are rarely an accurate measure of knowledge
        - Luck and whim is often at play (especially in multiple choice)
        - Professors tend to use information directly from lecture/ other course materials on exams (in which case the exams test your conditioning in their course materials, not in the depth of your understanding of the actual topic)
    - Learning things just to memorize them for an exam
        - Forgetting them immediately afterwards
    - Viability of cheating (in general) and GPT
        - They're often outlawed in situations where they ought to be used in the real world (i.e. collaboration on the job)
    - Graded/mandatory attendance
        - Less flexibility and freedom to the student
    - Letter grades are discrete
        - The difference between an A and an AB could be 0.01%
    - Grades are determined by how well you perform in comparison to your peers, not by how much you actually know
        - This falls apart when your class is skewed in either direction
        - This fosters a mindset of competition and outward focus in place of introspection
- Artificial boundaries, red tape
    - Prerequisite courses
        - Usually can't be tested out of
            - Could often be replaced with a brief independent study
                - But isn't, because it's out of the interest of the school ($$$)
        - Rarely are completely necessary (usually only a fraction of the material from the prereq is actually used)
    - Information hoarding
        - You must enroll in a course to get the course materials
            - The internet allows sharing all course materials publicly among the student body trivial, but universities purposely hide them for their selfish advantage and to the detriment of students
        - Professors often retain information
            - Lazily dispersing readings/homeworks/projects/assignments very shortly before the deadlines
                - This is a huge discouragement for students who want to get work done as early as possible
                - This encourages (and practically requires) procrastination
            - Not sharing lecture videos
                - Many professors who record videos only share them with disabled students or athletes with conflicts, in attempt to get more students to attend lecture
                    - This is yet another example of professors deciding for themselves what's best for students, taking the power and independence over learning out of the students' hands
    - Class-size limits
        - Typically governed by room size and faculty/TA grading capacity
        - Often mean deferring necessary prerequisites or classes of personal interest
            - This happens very frequently with popular classes
    - General education requirements
        - Completely arbitrary (why this specific number of literature credits?)
        - Not necessary for success or knowledge in the major
            - A distraction and unnecessary blockade in front of graduation
- Misaligned priorities
    - Professors are rewarded by research, not by teaching
        - There's no external incentive for professors to have positive personal interactions with undergraduates
        - The majority of the computational power of the greatest minds in the university is focused away from teaching
            - Improving and modernizing course material is a very low priority
    - Lectures are taught at the speed of the average student
        - This is sub-ideal for students on either side of average
    - Malevolent advisors
        - The priority of advisors is to keep you at the university for as long as possible, *not to help you achieve what's in your best interest*
            - This is fundamentally opposed to what an advisor should be
- Inefficiency
    - Courses move at a (very slow) fixed rate
        - There's no class on weekends, and you can't use your free time to advance more quickly in classes (except by taking more classes to graduate early)
    - The university imposes limits on how many credits you can take in a term
    - Taking a class often requires much more effort than learning all the material associated with the class
        - There is a lot of redundancy in lectures/homework/projects
            - This redundancy often takes the place of targeted independent study that could be filling in the blanks of what you don't know, rather than reiterating what you already do know
        - Classes often require learning nominal information (memorization) in order to make testing easier (it's easier to test memory than conceptual things)
            - This information is rarely important in the long-run and is often forgotten quickly
    - Classes allow much less capacity for practical, self-directed, and [lazy](/blog/lazy-vs-eager-learning) learning
        - Understanding practical applications is much more difficult and energy-consuming when eagerly learning (taking information without applying it)
            - It's much easier to get discouraged and lost when learning this way
        - Feedback loops are very loose (have to wait for grades instead having the instant feedback of running a program)

Many of these are true for school in general (not just college).
It's especially common for younger (i.e. elementary/middle school) students to "not like school"; I suspect that these reasons are core[^other] towards that feeling.
For a long time, I thought I didn't like school because I couldn't study the thing I was passionate about (computers), but as I began to take more and more computer classes once I got to college, I still felt the same frustrations, I realized that my dislike for school was only partly related with my disinterest in the subject matter, and was really about the bad abstraction at school's core.

[^other]: There are certainly other factors at play, like a repulsion from responsibility and difficulty, but I'm not convinced that those explain everything.

A lot of these problems are deeply embedded into the format of school and can't be fixed.
Many of them can be improved with great effort and resources, and a few can be improved with minimal effort, but doing so is a low priority for the people in power.

There are flaws for every system as big as a university, and I wouldn't be so critical if there weren't a better solution: learning on your own.
Being self-taught fixes the vast majority of these issues.
Unfortunately, it comes with problems of its own.

## Self-Taught: The Downsides

### Getting Hired

I'm a huge advocate for being self-taught: if you have the necessary passion, discipline, and direction, it's by far the most effective and enjoyable way to learn.
However, I'm very skeptical about about the scalability of independent-study as a complete replacement for college, particularly when it comes to starting a career.
The demand for junior developers isn't going up very quickly[^numbers], yet supply is at an all-time high: the CS major is rapidly increasing in popularity.
This makes getting hired a difficult feat, even for the most qualified applicants (those with degrees *and* experience).

[^numbers]: This makes a lot of sense considering how junior developers generally begin as a liability
(it takes time for them to get up to speed with the industry), and they have a tendency to hop jobs very quickly.

Furthermore, it's incredibly difficult to prove competence[^compel] gained outside of job-experience.
The résumé is the great equalizer on this front.
The standard format uses the following sections:

[^compel]: At least in a compelling enough way to land an interview.

- Education
- Experience
- Projects
- Skills

If you're self-taught, "education" and "experience" are mostly empty, so your bread-and-butter is "projects" and "skills".
"Skills" is generally ignored, though, unless there's something especially relevant to the position (in which case it's usually more of a non-negative impact than a hugely positive one), and "projects" section means very little,
as trivial projects look almost the same as very-impressive projects through the lens of three bullet points,
especially to recruiters and HR-people with minimal technical knowledge.

The only reliable way to get hired in software today is through networking, and if you don't have leads on that front, you're bound to be demoralized trying to find a job as a self-taught developer.

### Direction

It's also more likely for self-taught developers to have uncertainty about *what* they should learn, and to what degree they are making progress.
College is very good at setting a concrete path of things to study, and giving you regularly timed progress reports, reminding you you're on the right track.
When you're self-taught, you decide everything, and that flexibility is both an advantage and a weakness.
The degree to which this will be a problem is highly dependent on the person doing the studying, and the situation they're in.
Confidence and uncertainty must be balanced, which can be hard to get right.

Spending time becoming self-taught is not very socially acceptable: I suspect most parents will be more skeptical of this path over getting a college degree.
This adds an extra factor of pressure and self-doubt to what's already a tough sell.
Even with maximal time and effort, going from programming noob to employable is bound to take a few months (in most situations, it's probably more like a year): I'd imagine most people would run into some serious doubts within that time[^mentor].

[^mentor]: For this reason, if you do choose to take this path, it's highly advisable to have a mentor or some other technical support structure to avoid this pitfall.

## The Third Option

Luckily, the downsides of college can be lessened by spending less time there.
And in most situations, that's easy enough to do, especially when wielding the powers of self-teaching.

So here's the third option: go to college, but graduate as fast as possible.
Use the framework of college to decide what to learn[^more], teach it to yourself, then breeze through as many classes as possible in the shortest amount of time.

[^more]: Use it to decide a baseline of what to focus on (don't necessarily follow it dogmatically or use it as an excuse not to study other things).

### Advantages

Doing this decreases the downsides of going to college:
- Cheaper
- Less time dealing with the bad abstractions

It also removes all the disadvantages of being self-taught, namely:
- You escape with a diploma
- It's hard to lose direction (not know what to study/be uncertain) when you're in college
- Going to college gives you a network

### Disadvantages

It's a lot of work.
So is being self-taught.
This entire post assumes you're in for the work.
It's hard to get anything done if you don't want to work.

Less time for:
- Internships
- Friendships
- Relationships
- Leisure

### Logistics

Graduating early can be broken down into three steps:

1. Knowing what you must do to graduate
    - Find, read, and re-read the graduation requirements at your university
2. Making a plan for packing that in a short time
    - Come into college with as many credits as possible
    - Take the maximal number of credits each term (or even overload)
    - Take prerequisites as early as possible
    - Taking courses over the summer (or possibly the winter)
    - If you want good grades, have a plan to study before/during those classes
3. Executing it early enough
    - By far the hardest part
    - Getting college credit in high school

As an example, here's my [2-year plan at UW](/blog/uw-madison-computer-science/).
Here's a [video from a guy who managed to pull it off in 1 year](https://www.youtube.com/watch?v=Hab7xorie1o).

## Reservations

Speedrunning isn't for everybody: it probably doesn't make sense if you fall into any of the following categories.

- Not a recent high-school graduate/already in a non-software career
    - You're probably not trying to get a job right away, so being self-taught is more feasible and lower-risk
- Scholarship recipient / grant recipient / wealthy family
    - You don't have to worry about debt, so college has fewer downsides
- Not excited by your major
    - Don't do this for a money-grab: you'll quickly lose energy and patience
- Heavily reliant on the structure of classes to orient learning
    - It will be difficult to study outside of class, and taking many classes at once will be harder

## Preemptive Defense

There's a lot of college romanticism out there, and I foresee criticism towards speedrunning college, as it discourages many of the traditional aspects of the "college experience".
There's a popular notion that college ought to be a place of *high-level academic exploration*, *personal discovery* and *socialization*.
The overtones usually imply that these things should be done in a relaxed and low-pressure environment.

I don't completely disagree, but I'm much more realistic than that.
I think academic exploration (in the "deciding my major" sense) should be done before a financial burden is accepted.
Personal discovery is something that happens over time, usually in uncomfortable and uncertain circumstances (rarely in idyllic ones) - it isn't something to be bought through tuition.
While socialization is theoretically good, it should not take priority over the ultimate goal of education, especially when it establishes unhealthy habits and incontinent behavior.
And above all, college ought not to be a time of relaxation, but one of hard work and distinction, as it sets the stage for the remainder of a career and a life.
